{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/CoilData.csv')\n",
    "coils = pd.read_csv('../data/output.csv')\n",
    "coil_list = list(map(int,list(coils.columns)))\n",
    "lst = []\n",
    "for i in df['coil']:\n",
    "    if i in coil_list:\n",
    "        lst.append(1)\n",
    "    else:\n",
    "        lst.append(0)\n",
    "df['contracted'] = lst\n",
    "df['analyse_main'] = [i[0:4] for i in df['analyse']]\n",
    "dummies_analyse_main = pd.get_dummies(df['analyse_main'], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad coils exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-24f7839fb846>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_c['analyse_main'] = [i[:-2] for i in bad_c['analyse']]\n"
     ]
    }
   ],
   "source": [
    "bad_c = df[df['contracted']==1]\n",
    "bad_c['analyse_main'] = [i[:-2] for i in bad_c['analyse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20% of most recurring main categories count for 77.17055971793741% of the total number of contracted coils\n"
     ]
    }
   ],
   "source": [
    "len_main = len(bad_c.analyse_main.value_counts())\n",
    "top_ninety_percentile = bad_c.analyse_main.value_counts(ascending=False).head(int(len_main/5)).sum()\n",
    "the_rest = bad_c.analyse_main.value_counts(ascending=True).head(len_main - int(len_main/5)).sum()\n",
    "ratio = 1 - (the_rest/top_ninety_percentile)\n",
    "\n",
    "print(f'The top 20% of most recurring main categories count for {ratio * 100}% of the total number of contracted coils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-db7c74926d19>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_c['is_in_main_top_main_categories'] = in_main_category\n"
     ]
    }
   ],
   "source": [
    "# get list of 20% most recurring categories\n",
    "top_bad_main_categories = bad_c.analyse_main.value_counts(ascending=False).head(int(len_main/5)).index.tolist()\n",
    "\n",
    "# add column if coil in category\n",
    "in_main_category = []\n",
    "for coil in bad_c['analyse']:\n",
    "    if coil[:-2] in top_bad_main_categories:\n",
    "        in_main_category.append(1)\n",
    "    else:\n",
    "        in_main_category.append(0)\n",
    "\n",
    "bad_c['is_in_main_top_main_categories'] = in_main_category\n",
    "bad_coil_list = list(bad_c.coil[bad_c['is_in_main_top_main_categories'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Hardness_1  Hardness_2   Width  Thickness Thickness profile    c    mn  \\\n",
      "0       10003         101  1302.1       4.36                31  355  2162   \n",
      "1       10123         101  1282.3       4.37                35  551  1985   \n",
      "2       10040         102  1297.4       4.43                25  457  1895   \n",
      "3       10243         102  1295.2       4.44                28  697  2008   \n",
      "4       10012         100  1293.3       3.95                26  477  1936   \n",
      "\n",
      "    si  nb    p    s   al   ma  b   n  ti   cr  va  mo  contracted  \n",
      "0   49   0  133  143  304  291  1  34   6  302   0  25           1  \n",
      "1  101   0  118   90  395  384  1  33  12  189  25   7           0  \n",
      "2   60   0  108  115  476  463  1  20  11  288   0  40           0  \n",
      "3   69   0  139   98  306  296  1  21   9  253   0   9           0  \n",
      "4   52   0  112  121  340  329  1  28   8  297   0  23           0  \n"
     ]
    }
   ],
   "source": [
    "# removing columns\n",
    "coil_list = df.coil\n",
    "df = df.drop(columns=['coil', 'analyse', 'analyse_main', 'furnace Number', 'Temperature before finishing mill', \n",
    "                      'Temperature after finishing mill'])\n",
    "data = df.copy()\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# replace string values in Thickness profile column\n",
    "data['Thickness profile'] = data['Thickness profile'].apply(lambda x: x.replace('*******', ''))\n",
    "data = data.replace('', np.nan, regex=True).dropna().astype(float)\n",
    "data = data[data['Thickness profile'] >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Hardness_1 Hardness_2     Width Thickness Thickness profile\n",
      "0      -0.864475  -0.593122  0.143819  0.748022          0.910067\n",
      "1      -0.775493  -0.593122  0.075519  0.757278          1.273254\n",
      "2      -0.837039  -0.533462  0.127606  0.812812          0.365286\n",
      "3      -0.686510  -0.533462  0.120018  0.822067          0.637677\n",
      "4      -0.857802  -0.652781  0.113463  0.368543          0.456083\n",
      "...          ...        ...       ...       ...               ...\n",
      "56506  -1.029093  -0.772100 -0.863782 -1.584390         -0.088698\n",
      "56507  -1.015746  -0.772100 -0.861713 -1.584390         -0.179494\n",
      "56508  -0.967547  -0.712441 -0.868612 -1.584390         -0.361088\n",
      "56509  -0.837780  -0.652781 -0.854469 -1.584390          0.002099\n",
      "56510  -0.984602  -0.772100 -0.868267 -1.584390         -0.451885\n",
      "\n",
      "[56511 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Standardize data\n",
    "# scaler = StandardScaler()\n",
    "# selection_standardize = data.iloc[:,0:5]\n",
    "# list_columns = selection_standardize.columns\n",
    "# scaled_selection = pd.DataFrame(data=scaler.fit_transform(selection_standardize), columns=[list_columns])\n",
    "# print(scaled_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/arcelor_mittal_env/lib/python3.8/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/anaconda3/envs/arcelor_mittal_env/lib/python3.8/site-packages/pandas/core/indexing.py:1783: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "# Transform to log data and replace -inf values with min value != 0 \n",
    "# in each column and get the log of that number divided by 1000\n",
    "log_selection = data.iloc[:,0:19]\n",
    "for column in list(log_selection.columns):\n",
    "    min_value_per_column = min(i for i in log_selection.loc[:,column] if i > 0)\n",
    "    log_selection.loc[:,column] = np.log(log_selection.loc[:,column]).replace(-np.inf, np.log(min_value_per_column/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Join dataframes and adding OneHotEncoding for Categorical values of 'Analyse' column\n",
    "# data = scaled_selection.join(log_selection).join(dummies_analyse_main).join(df['contracted']).join(coil_list).dropna()\n",
    "data = log_selection.join(dummies_analyse_main).join(df['contracted']).join(coil_list).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection and partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2261\n",
      "                          0         1          2         3         4\n",
      "Hardness_1         9.210640  9.213635   9.200189  9.254453  9.222467\n",
      "Hardness_2         4.615121  4.605170   4.595120  4.653960  4.615121\n",
      "Width              7.171734  7.375882   7.344525  7.242726  7.170196\n",
      "Thickness          1.472472  1.420696   1.337629  0.924259  0.841567\n",
      "Thickness profile  3.433987  1.609438 -11.036691  2.197225  2.484907\n",
      "...                     ...       ...        ...       ...       ...\n",
      "TB53               0.000000  0.000000   0.000000  0.000000  0.000000\n",
      "TB61               0.000000  0.000000   0.000000  0.000000  0.000000\n",
      "TB63               0.000000  0.000000   0.000000  0.000000  0.000000\n",
      "TB71               0.000000  0.000000   0.000000  0.000000  0.000000\n",
      "contracted         1.000000  1.000000   1.000000  1.000000  1.000000\n",
      "\n",
      "[242 rows x 5 columns]\n",
      "                          0         1          2         3         4\n",
      "Hardness_1         9.248310  9.246479   9.448727  9.212139  9.218309\n",
      "Hardness_2         4.644391  4.644391   4.820282  4.605170  4.615121\n",
      "Width              7.531284  6.772737   7.042898  7.236987  7.265150\n",
      "Thickness          1.324419  0.943906   1.068153  1.217876  1.217876\n",
      "Thickness profile  2.397895  2.890372 -11.036691  2.397895  2.708050\n",
      "...                     ...       ...        ...       ...       ...\n",
      "TB53               0.000000  0.000000   0.000000  0.000000  0.000000\n",
      "TB61               0.000000  0.000000   0.000000  0.000000  0.000000\n",
      "TB63               0.000000  0.000000   0.000000  0.000000  0.000000\n",
      "TB71               0.000000  0.000000   0.000000  0.000000  0.000000\n",
      "contracted         0.000000  0.000000   0.000000  0.000000  0.000000\n",
      "\n",
      "[242 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Making balanced datasets\n",
    "in_bad_coil_list = []\n",
    "for coil in data['coil']:\n",
    "    if coil in bad_coil_list:\n",
    "        in_bad_coil_list.append(1)\n",
    "    else:\n",
    "        in_bad_coil_list.append(0)\n",
    "data['in_bad_coil_list'] = in_bad_coil_list\n",
    "df_bad_coils = data[data['in_bad_coil_list']==1].reset_index().drop(columns=['coil',\n",
    "                                                                             'in_bad_coil_list',\n",
    "                                                                             'index'])\n",
    "len_bad_coil_list = len(df_bad_coils)\n",
    "print(len_bad_coil_list)\n",
    "print(df_bad_coils.head().T)\n",
    "\n",
    "df_good_coils = data[data.contracted == 0].sample(len_bad_coil_list).reset_index().drop(columns=['coil',\n",
    "                                                                                                 'in_bad_coil_list',\n",
    "                                                                                                 'index'])\n",
    "df_good_coils.to_csv('df_good_coils.csv', index=True, header=True)\n",
    "df_good_coils = pd.read_csv('df_good_coils.csv').drop(columns=['Unnamed: 0']).dropna()\n",
    "print(df_good_coils.head().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2261 2261\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# concat even number of good and bad coils in df and reshuffle the dataframe so we randomize the the data when we\n",
    "# split it. \n",
    "df_balanced_coils = pd.concat([df_good_coils, df_bad_coils]).sample(frac=1).reset_index(drop=True).dropna()\n",
    "X = df_balanced_coils.iloc[:,:-2]\n",
    "y = df_balanced_coils.contracted\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(df_bad_coils), len(df_good_coils))\n",
    "\n",
    "\n",
    "print(np.any(np.isnan(df_balanced_coils)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "svm = svm.SVC(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set grid search params\n",
    "max_depth_range = np.arange(3,15,3)\n",
    "max_iter = np.arange(100,1000,100)\n",
    "\n",
    "grid_params_lr = [{'multi_class': ['auto', 'ovr', 'multinomial'],\n",
    "                  'max_iter': [1000]}]\n",
    "\n",
    "grid_params_dt = [{'criterion': ['gini', 'entropy'],\n",
    "                  'max_depth': max_depth_range}]\n",
    "\n",
    "grid_params_rf = [{'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': max_depth_range,\n",
    "        'min_samples_split': max_depth_range}]\n",
    "\n",
    "grid_params_svm = [{'kernel': ['linear', 'rbf'], \n",
    "        'C': max_depth_range}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = GridSearchCV(lr,\n",
    "            param_grid=grid_params_lr,\n",
    "            scoring='accuracy',\n",
    "            cv=3)\n",
    "\n",
    "DT = GridSearchCV(dt,\n",
    "                 param_grid=grid_params_dt,\n",
    "                 scoring='accuracy',\n",
    "                 cv=3)\n",
    "\n",
    "RF = GridSearchCV(rf,\n",
    "                 param_grid=grid_params_rf,\n",
    "                 scoring='accuracy',\n",
    "                 cv=3)\n",
    "\n",
    "SVM = GridSearchCV(svm,\n",
    "                  param_grid=grid_params_svm,\n",
    "                  scoring='accuracy',\n",
    "                  cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [LR, DT, RF, SVM]\n",
    "\n",
    "# Creating a dict for our reference\n",
    "grid_dict = {0: 'Logistic Regression',\n",
    "            1: 'Decision Tree Classifier',\n",
    "            2: 'Random Forest Classifier',\n",
    "            3: 'Support Vector Machine'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model optimizations...\n",
      "\n",
      "Estimator: Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/arcelor_mittal_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/arcelor_mittal_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/arcelor_mittal_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/arcelor_mittal_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/arcelor_mittal_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/arcelor_mittal_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are : {'max_iter': 1000, 'multi_class': 'multinomial'}\n",
      "Best training accuracy: 0.841\n",
      "Test set accuracy score for best params: 0.855 \n",
      "\n",
      "Estimator: Decision Tree Classifier\n",
      "Best params are : {'criterion': 'gini', 'max_depth': 9}\n",
      "Best training accuracy: 0.841\n",
      "Test set accuracy score for best params: 0.860 \n",
      "\n",
      "Estimator: Random Forest Classifier\n",
      "Best params are : {'criterion': 'gini', 'max_depth': 12, 'min_samples_split': 3}\n",
      "Best training accuracy: 0.858\n",
      "Test set accuracy score for best params: 0.873 \n",
      "\n",
      "Estimator: Support Vector Machine\n",
      "Best params are : {'C': 9, 'kernel': 'linear'}\n",
      "Best training accuracy: 0.843\n",
      "Test set accuracy score for best params: 0.848 \n",
      "\n",
      "Classifier with best test set accuracy: Random Forest Classifier\n",
      "\n",
      "Saved Random Forest Classifier grid search pipeline to file: best_grid_search_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "# Fit the grid search objects\n",
    "print('Performing model optimizations...')\n",
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    gs.fit(x_train, y_train)\n",
    "    print('Best params are : %s' % gs.best_params_)\n",
    "    # Best training data accuracy\n",
    "    print('Best training accuracy: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    y_pred = gs.predict(x_test)\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set accuracy score for best params: %.3f ' % accuracy_score(y_test, y_pred))\n",
    "    # Track best (highest test accuracy) model\n",
    "    if accuracy_score(y_test, y_pred) > best_acc:\n",
    "        best_acc = accuracy_score(y_test, y_pred)\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "print('\\nClassifier with best test set accuracy: %s' % grid_dict[best_clf])\n",
    "\n",
    "# Save best grid search pipeline to file\n",
    "dump_file = 'best_grid_search_pipeline.pkl'\n",
    "joblib.dump(best_gs, dump_file, compress=1)\n",
    "print('\\nSaved %s grid search pipeline to file: %s' % (grid_dict[best_clf], dump_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([   0,    1,    4,    6,    8,    9,   10,   11,   12,   13,\\n            ...\\n            4509, 4511, 4512, 4513, 4514, 4515, 4516, 4518, 4520, 4521],\\n           dtype='int64', length=3617)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d2590ebe7d8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/arcelor_mittal_env/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2906\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2907\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2908\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2910\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/arcelor_mittal_env/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/arcelor_mittal_env/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([   0,    1,    4,    6,    8,    9,   10,   11,   12,   13,\\n            ...\\n            4509, 4511, 4512, 4513, 4514, 4515, 4516, 4518, 4520, 4521],\\n           dtype='int64', length=3617)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Now we can create k train-test splits using KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Using KFold instead of calling multiple times train_test_split to ensure that each\n",
    "# sample goes into a single split only\n",
    "kf = KFold(n_splits=5, random_state=45, shuffle=True)\n",
    "\n",
    "split = 0\n",
    "scores = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    result = single_grid_search(X_train, y_train)\n",
    "    \n",
    "    decision_tree = result.best_estimator_\n",
    "    score = decision_tree.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "    print(\"### Split {}: Accuracy is {:.2f}% ###\".format(split := split + 1, score*100))\n",
    "    \n",
    "print(\"The mean generalization accuracy of the model is {:.2f}% (+/- {:.2f}%)\".format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of GridSearchCV parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
